<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <!-- Open Graph meta tags (Facebook/LinkedIn) -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="Enhancing Jailbreak Attacks via Compliance-Refusal-Based Initialization"/>
  <meta property="og:description" content="Demonstration of CRI for gradient-based jailbreak attacks on LLMs."/>
  <meta property="og:url" content="URL_OF_THE_WEBSITE"/>
  <meta property="og:image" content="static/images/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>

  <!-- No Twitter meta tags included -->

  <!-- Keywords for indexing -->
  <meta name="keywords" content="Jailbreak Attacks, Compliance Refusal Initialization, GCG, LLM, CRI, HarmBench">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Enhancing Jailbreak Attacks via Compliance-Refusal-Based Initialization</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <!-- Bulma and custom CSS -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <!-- JS scripts -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>

  <!-- Hero / Title Section -->
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">
              Enhancing Jailbreak Attacks via Compliance-Refusal-Based Initialization
            </h1>

            <!-- Authors -->
            <div class="is-size-5 publication-authors" style="margin-top: 1.5em;">
              <span class="author-block">
                <a href="mailto:amitlevi@campus.technion.ac.il" target="_blank">Amit Levi</a><sup>*1</sup>
              </span>
              <span class="author-block">
                <a href="mailto:romh@campus.technion.ac.il" target="_blank">Rom Himelstein</a><sup>*2</sup>
              </span>
              <span class="author-block">
                Yaniv Nemcovsky<sup>1</sup>
              </span>
              <span class="author-block">
                Avi Mendelson<sup>1</sup>
              </span>
              <span class="author-block">
                Chaim Baskin<sup>3</sup>
              </span>
            </div>

            <!-- Affiliations -->
            <div class="is-size-6 publication-authors" style="margin-top: 1em;">
              <p>
                <sup>1</sup>Department of Computer Science, Technion - Israel Institute of Technology<br>
                <sup>2</sup>Department of Data and Decision Science, Technion - Israel Institute of Technology<br>
                <sup>3</sup>School of Electrical and Computer Engineering, Ben-Gurion University of the Negev
              </p>
              <p class="eql-cntrb" style="margin-top: 1em;">
                <small><sup>*</sup>Indicates Equal Contribution</small>
              </p>
            </div>

            <!-- Links (ArXiv, GitHub, etc.) -->
            <div class="publication-links" style="margin-top: 1.5em;">
              <span class="link-block">
                <a href="https://arxiv.org/pdf/YOUR_ARXIV_PAPER_ID.pdf" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fas fa-file-pdf"></i></span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fas fa-file-pdf"></i></span>
                  <span>Supplementary</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/REPO_HERE" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fab fa-github"></i></span>
                  <span>Code</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/YOUR_ARXIV_PAPER_ID" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="ai ai-arxiv"></i></span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div> <!-- column -->
        </div> <!-- columns -->
      </div> <!-- container -->
    </div> <!-- hero-body -->
  </section>

  <!-- Abstract Section -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Jailbreak attacks aim to exploit large language
              models (LLMs) and pose a significant threat to
              their proper conduct; they seek to bypass models’ safeguards
              and often provoke transgressive behaviors. However, existing
              automatic jailbreak attacks require extensive computational resources
              and are prone to converge on suboptimal solutions.
              In this work, we propose <strong>Compliance Refusal
              Initialization (CRI)</strong>, a novel, attack-agnostic
              framework that efficiently initializes the optimization
              in the proximity of the compliance subspace
              of harmful prompts. By narrowing the initial gap
              to the adversarial objective, CRI substantially improves
              adversarial success rates (ASR) and drastically
              reduces computational overhead—often
              requiring just a single optimization step. We evaluate
              CRI on the widely-used <em>AdvBench</em> dataset
              over the standard jailbreak attacks of <em>GCG</em> and
              <em>AutoDAN</em>. Results show that CRI boosts ASR
              and decreases the median steps to success by up
              to <em>×60</em>.
            </p>
            <p>
              We also tested the robustness of one of the DeepSeek models by running
              the <strong>GCG jailbreak attack</strong> with CRI on the
              <strong>HarmBench</strong> benchmark, demonstrating faster convergence
              and high adversarial success.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Introduction Section -->
  <section class="section hero is-white">
    <div class="container is-max-desktop content has-text-justified">
      <h2 class="title is-3">Introduction</h2>
      <p>
        LLMs have recently emerged with extraordinary capabilities and have rapidly become integral to
        numerous fields, transforming everyday tasks such as text generation, image generation, and complex
        decision-making tasks. Despite their advantages, the widespread deployment of LLMs has unveiled
        critical security vulnerabilities, making them susceptible to malicious misuse. A common strategy to
        enhance the robustness of LLMs is alignment training, effectively separating the input space into
        <em>Compliance</em> and <em>Refusal</em> subspaces. However, this segmentation has inadvertently
        fueled adversarial jailbreak attacks that transform inputs to coerce models into providing unwanted
        compliance outputs.
      </p>
      <p>
        In our work, we leverage the distinction between refusal and compliance behaviors. By fine-tuning
        the initialization of jailbreak prompts, we show that CRI can significantly enhance attack success rates
        while reducing the computational overhead. Our experimental results indicate that the alignment process
        itself may induce vulnerable directions in the latent representation of LLMs, which attackers can exploit.
        Understanding and exposing these vulnerabilities is a crucial step toward developing more secure and
        robust language models.
      </p>
      <p>
        The remainder of this document details our CRI approach, experimental findings, and discussions on
        broader impacts and ethical considerations.
      </p>
    </div>
  </section>

  <!-- New Section: Fast LLM Jailbreak Robustness Test -->
  <section class="section hero is-light">
    <div class="container is-max-desktop content has-text-justified">
      <h2 class="title is-3">Fast LLM Jailbreak Robustness Test with GCG + CRI</h2>
      <p>
        To further demonstrate the effectiveness of CRI, we conducted a fast LLM jailbreak robustness test 
        on one of the <strong>DeepSeek</strong> models using the <strong>GCG</strong> jailbreak attack on the 
        <strong>HarmBench</strong> benchmark. Below is an overview of how we set up and ran our experiments:
      </p>
      <ol>
        <li>
          <strong>Environment Setup:</strong> We prepared the HarmBench dataset 
          and the DeepSeek model environment. Ensuring correct dependencies and GPU availability 
          is critical for efficient batched inference.
        </li>
        <li>
          <strong>Applying CRI:</strong> Before launching the jailbreak attack, we generated a 
          set of CRI-based initial transformations. These transformations were pre-trained once 
          (using a few samples from HarmBench), aiming to place the prompts closer to the 
          <em>compliance subspace</em>.
        </li>
        <li>
          <strong>Running the GCG Attack:</strong> We then ran GCG with the CRI-initialized prompts. 
          GCG iteratively refines the prompt by leveraging gradient signals. Due to the more 
          informative starting point from CRI, GCG converged more quickly toward successful jailbreaks, 
          reducing total optimization steps.
        </li>
        <li>
          <strong>Measuring Robustness:</strong> We measured adversarial success rates (ASR), 
          the number of steps to success, and overall computational overhead. The DeepSeek model exhibited 
          faster convergence to harmful completions compared to standard or random initializations, 
          highlighting CRI’s efficiency.
        </li>
      </ol>
      <p>
        Our results demonstrated that, even under challenging conditions, CRI-augmented GCG maintained 
        a high success rate against the DeepSeek model on HarmBench, requiring fewer steps 
        on average to achieve a jailbreak. This underscores CRI’s potential for fast, systematic 
        robustness evaluation in future LLM security research.
      </p>
    </div>
  </section>

  <!-- Image / Framework Section -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <h2 class="title">Framework Overview</h2>
        <figure class="image">
          <!-- Replace with your actual path and image -->
          <img src="static/images/framework_overview.png" alt="CRI Framework Overview">
        </figure>
        <p class="subtitle has-text-centered" style="margin-top: 1em;">
          Figure: Illustration of CRI’s attack-agnostic initialization approach for jailbreaking.
        </p>
      </div>
    </div>
  </section>

  <!-- Image / Results Section -->
  <section class="hero is-light is-small">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <h2 class="title">Main Results Visualization</h2>
        <figure class="image">
          <!-- Replace with your actual path and image -->
          <img src="static/images/main_results.png" alt="Main Results Visualization">
        </figure>
        <p class="subtitle has-text-centered" style="margin-top: 1em;">
          Figure: CRI significantly reduces the steps required to achieve jailbreak while boosting ASR.
        </p>
      </div>
    </div>
  </section>

  <!-- Poster / PDF Embed (Optional) -->
  <section class="hero is-small is-white">
    <div class="hero-body">
      <div class="container">
        <h2 class="title">Poster</h2>
        <!-- Example embedded PDF, update if you want to display a poster or a paper excerpt -->
        <iframe src="static/pdfs/sample.pdf" width="100%" height="550"></iframe>
      </div>
    </div>
  </section>

  <!-- BibTeX citation (Optional) -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
<pre><code>@misc{levi2025cri,
  title       = {Enhancing Jailbreak Attacks via Compliance-Refusal-Based Initialization},
  author      = {Levi, Amit and Himelstein, Rom and Nemcovsky, Yaniv and Mendelson, Avi and Baskin, Chaim},
  year        = {2025},
  howpublished= {Preprint},
  url         = {https://arxiv.org/abs/YOUR_ARXIV_PAPER_ID}
}
</code></pre>
    </div>
  </section>

  <!-- Footer -->
  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This page was built using the 
              <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">
                Academic Project Page Template
              </a> 
              which was adopted from the 
              <a href="https://nerfies.github.io" target="_blank">Nerfies</a> 
              project page. You are free to borrow the source code of this website; we just ask that you link back 
              to this page in the footer. <br>
              This website is licensed under a 
              <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">
                Creative Commons Attribution-ShareAlike 4.0 International License
              </a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- (Optional) Statcounter or other analytics can go here -->

</body>
</html>



