<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <!-- Open Graph meta tags (Facebook/LinkedIn) -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="Enhancing Jailbreak Attacks via Compliance-Refusal-Based Initialization"/>
  <meta property="og:description" content="Demonstration of CRI for gradient-based jailbreak attacks on LLMs."/>
  <meta property="og:url" content="URL_OF_THE_WEBSITE"/>
  <meta property="og:image" content="static/images/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>

  <!-- No Twitter meta tags included -->

  <!-- Keywords for indexing -->
  <meta name="keywords" content="Jailbreak Attacks, Compliance Refusal Initialization, GCG, LLM, CRI, HarmBench">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Enhancing Jailbreak Attacks via Compliance-Refusal-Based Initialization</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <!-- Bulma and custom CSS -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <!-- JS scripts -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>

  <!-- Hero / Title Section -->
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">
              Enhancing Jailbreak Attacks via Compliance-Refusal-Based Initialization
            </h1>

            <!-- Authors -->
            <div class="is-size-5 publication-authors" style="margin-top: 1.5em;">
              <span class="author-block">
                <a href="mailto:amitlevi@campus.technion.ac.il" target="_blank">Amit LeVi</a><sup>*1</sup>
              </span>
              <span class="author-block">
                <a href="mailto:romh@campus.technion.ac.il" target="_blank">Rom Himelstein</a><sup>*2</sup>
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?hl=en&user=Tzi_svcAAAAJ" target="_blank">Yaniv Nemcovsky</a><sup>*</sup><sup>1</sup>

              </span>
              <span class="author-block"> 
               <a href="https://scholar.google.com/citations?hl=en&view_op=search_authors&mauthors=Avi+Mendelson&btnG=#:~:text=My%20library-,Avi%20Mendelson,-Electrical%20Engineering%20and" target="_blank">Avi Mendelson</a><sup>*</sup><sup>1</sup>

              </span>
              <span class="author-block">
            <a href="https://scholar.google.com/citations?hl=en&user=lfWCxJYAAAAJ" target="_blank">Chaim Baskin</a><sup>*</sup><sup>1</sup>
              </span>
            </div>

            <!-- Affiliations -->
            <div class="is-size-6 publication-authors" style="margin-top: 1em;">
              <p>
                <sup>1</sup>Department of Computer Science, Technion - Israel Institute of Technology<br>
                <sup>2</sup>Department of Data and Decision Science, Technion - Israel Institute of Technology<br>
                <sup>3</sup>School of Electrical and Computer Engineering, Ben-Gurion University of the Negev
              </p>
              <p class="eql-cntrb" style="margin-top: 1em;">
                <small><sup>*</sup>Indicates Equal Contribution</small>
              </p>
            </div>

            <!-- Links (ArXiv, GitHub, etc.) -->
            <div class="publication-links" style="margin-top: 1.5em;">
              <span class="link-block">
                <a href="https://arxiv.org/pdf/YOUR_ARXIV_PAPER_ID.pdf" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fas fa-file-pdf"></i></span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/wr0om/Compliance-Refusal-Initialization" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fab fa-github"></i></span>
                  <span>Code</span>
                </a>
              </span>
            </div>
          </div> <!-- column -->
        </div> <!-- columns -->
      </div> <!-- container -->
    </div> <!-- hero-body -->
  </section>

  <!-- Abstract Section -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Jailbreak attacks aim to exploit large language
              models (LLMs) and pose a significant threat to
              their proper conduct; they seek to bypass mod-
              els’ safeguards and often provoke transgressive
              behaviors. However, existing automatic jailbreak
              attacks require extensive computational resources
              and are prone to converge on suboptimal solutions.
              In this work, we propose <strong>Compliance Refusal
              Initialization (CRI)</strong>, a novel, attack-agnostic
              framework that efficiently initializes the optimization
              in the proximity of the compliance subspace
              of harmful prompts. By narrowing the initial gap
              to the adversarial objective, CRI substantially im-
              proves adversarial success rates (ASR) and dras-
              tically reduces computational overhead—often
              requiring just a single optimization step. We eval-
              uate CRI on the widely-used <em>AdvBench</em> dataset
              over the standard jailbreak attacks of <em>GCG</em> and
              <em>AutoDAN</em>. Results show that CRI boosts ASR
              and decreases the median steps to success by up
              to <em>×60</em>.
            </p>
            <p>
              We also tested the robustness of one of the DeepSeek models by running
              the <strong>GCG jailbreak attack</strong> with CRI on the
              <strong>HarmBench</strong> benchmark, demonstrating faster convergence
              and high adversarial success.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Refusal & Compliance Part vRefusal (AFTER ABSTRACT) -->
  <section class="section hero is-white">
    <div class="container is-max-desktop content has-text-justified">
      <h2 class="title is-4">Compliance & Refusal</h2>
      <!-- Embedding the PNG image -->
      <figure class="image">
        <img src="static/images/refusal.png" alt="Refusal-Compliance Visualization">
      </figure>
      <p class="subtitle has-text-centered" style="margin-top: 1em;">
        Visualization highlighting the “Compliance-Refusal” direction and its overlapping jailbreak embeddings.
      </p>
    </div>
  </section>

  <!-- Introduction Section -->
  <section class="section hero is-white">
    <div class="container is-max-desktop content has-text-justified">
      <h2 class="title is-3">Introduction</h2>
      <p>
        LLMs have recently emerged with extraordinary capabilities and have rapidly become integral to
        numerous fields, transforming everyday tasks such as text generation, image generation, and complex
        decision-making tasks. Despite their advantages, the widespread deployment of LLMs has unveiled
        critical security vulnerabilities, making them susceptible to malicious misuse. A common strategy to
        enhance the robustness of LLMs is alignment training, effectively separating the input space into
        <em>Compliance</em> and <em>Refusal</em> subspaces. However, this segmentation has inadvertently
        fueled adversarial jailbreak attacks that transform inputs to coerce models into providing unwanted
        compliance outputs.
      </p>
      <p>
        In our work, we leverage the distinction between refusal and compliance behaviors. By fine-tuning
        the initialization of jailbreak prompts, we show that CRI can significantly enhance attack success rates
        while reducing the computational overhead. Our experimental results indicate that the alignment process
        itself may induce vulnerable directions in the latent representation of LLMs, which attackers can exploit.
        Understanding and exposing these vulnerabilities is a crucial step toward developing more secure and
        robust language models.
      </p>
      <p>
        The remainder of this document details our CRI approach, experimental findings, and discussions on
        broader impacts and ethical considerations.
      </p>
    </div>
  </section>

  <!-- Framework (AFTER INTRO) -->
  <section class="section hero is-light">
    <div class="container is-max-desktop content has-text-justified">
      <h2 class="title is-3">Framework</h2>
      <figure class="image">
        <img src="static/images/framework.png" alt="CRI Framework Overview">
      </figure>
      <p class="subtitle has-text-centered" style="margin-top: 1em;">
        A high-level overview of our Compliance-Refusal-Based Initialization (CRI) process.
      </p>
    </div>
  </section>

  <!-- Overall Results -->
  <section class="section hero is-light">
    <div class="container is-max-desktop content has-text-justified">
      <h2 class="title is-3">Overall Results</h2>
      <p>
        We summarize our findings across AdvBench, demonstrating that CRI consistently
        reduces the optimization steps required to achieve a successful jailbreak while elevating the
        adversarial success rates. These results reinforce the practical impact of CRI and emphasize
        potential vulnerabilities in alignment-trained LLMs.
      </p>
    </div>
  </section>

  <!-- Screenshot after Overall Results -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <h2 class="title"></h2>
        <figure class="image">
          <img src="static/images/Screenshot 2025-02-14 at 1.03.54.png" alt="Overall Results Screenshot">
        </figure>
        <p class="subtitle has-text-centered" style="margin-top: 1em;">
          Aggregated performance metrics showcasing CRI’s efficacy across various scenarios.
        </p>
      </div>
    </div>
  </section>


      <!-- Screenshot after Overall Results -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <h2 class="title"></h2>
        <figure class="image">
          <img src="static/images/asr.png" alt="Overall Results Screenshot">
        </figure>
        <p class="subtitle has-text-centered" style="margin-top: 1em;">
          ASR to steps.
        </p>
      </div>
    </div>
  </section>

  
    <!-- Screenshot after Overall Results -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <h2 class="title"></h2>
        <figure class="image">
          <img src="static/images/Loss.png" alt="Overall Results Screenshot">
        </figure>
        <p class="subtitle has-text-centered" style="margin-top: 1em;">
          Loss to steps.
        </p>
      </div>
    </div>
  </section>
  

  <!-- Fast LLM Jailbreak Robustness Test (Moved After Results) -->
  <section class="section hero is-white">
    <div class="container is-max-desktop content has-text-justified">
      <h2 class="title is-3">Fast LLM Jailbreak Robustness Test with GCG + CRI (HarmBench in minutes)</h2>
      <p>
        To further demonstrate the effectiveness of CRI, we conducted a <strong>fast LLM jailbreak robustness test</strong>
        on one of the <strong>DeepSeek</strong> models using the <strong>GCG + CRI</strong> jailbreak attack on the 
        <strong>HarmBench</strong> benchmark. Below is an overview of how we set up and ran our experiments:
      </p>
      <ol>
        <li>
          <strong>Applying CRI:</strong> Before launching the jailbreak attack, we generated a 
          set of CRI-based initial transformations. These transformations were pre-trained once 
          (using a few samples from HarmBench), aiming to place the prompts closer to the 
          <em>compliance subspace</em>.
        </li>
        <li>
          <strong>Running the GCG Attack:</strong> We then ran GCG with the CRI-initialized prompts. 
          GCG iteratively refines the prompt by leveraging gradient signals. Due to the more 
          informative starting point from CRI, GCG converged more quickly toward successful jailbreaks, 
          reducing total optimization steps.
        </li>
        <li>
          <strong>Measuring Robustness:</strong> We measured adversarial success rates (ASR), 
          the number of steps to success, and overall computational overhead. We demonstrate this Adversarial test on DeepSeek model, using CRI(ours) init exhibited 
          faster convergence to harmful completions compared to standard or random initializations, 
          highlighting CRI’s efficiency.
        </li>
      </ol>
      <p>
        This underscores CRI’s potential for fast, systematic robustness evaluation in future LLM security research.
      </p>
    </div>
  </section>

  <!-- Screenshot after Fast LLM Jailbreak -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <h2 class="title">AutoDan + CRI +  Prompt Injection(target) </h2>
        <figure class="image">
          <img src="static/images/Screenshot 2025-02-14 at 1.00.03.png" alt="Fast Jailbreak Screenshot">
        </figure>
        <p class="subtitle has-text-centered" style="margin-top: 1em;">
          Example for Jailbreak output
        </p>
      </div>
    </div>
  </section>

  <!-- BibTeX citation (Optional) -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
<pre><code>@misc{levi2025cri,
  title       = {Enhancing Jailbreak Attacks via Compliance-Refusal-Based Initialization},
  author      = {Levi, Amit and Himelstein, Rom and Nemcovsky, Yaniv and Mendelson, Avi and Baskin, Chaim},
  year        = {2025},
  howpublished= {Preprint},
  url         = {https://arxiv.org/abs/YOUR_ARXIV_PAPER_ID}
}
</code></pre>
    </div>
  </section>

  <!-- Footer -->
  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This page was built using the 
              <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">
                Academic Project Page Template
              </a> 
              which was adopted from the 
              <a href="https://nerfies.github.io" target="_blank">Nerfies</a> 
              project page. You are free to borrow the source code of this website; we just ask that you link back 
              to this page in the footer. <br>
              This website is licensed under a 
              <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">
                Creative Commons Attribution-ShareAlike 4.0 International License
              </a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- (Optional) Analytics scripts can go here -->

</body>
</html>


